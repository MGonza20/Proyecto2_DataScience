{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargando el dataset\n",
    "data = pd.read_csv('./data/train_clean.csv')\n",
    "\n",
    "# Mostrando las primeras filas del dataset para tener una idea del contenido\n",
    "data.head()\n",
    "\n",
    "data['extent'] = data['extent']/ 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Proyectos\\VIII\\DATA\\Proyecto2_DataScience\\myenv\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((20856, 10), (5215, 10))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Seleccionando las características y la variable objetivo\n",
    "X = data[['growth_stage', 'damage']]\n",
    "y = data['extent']\n",
    "\n",
    "# Realizando one-hot encoding para las características categóricas\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "\n",
    "# Dividiendo el conjunto de datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20856, 10]), torch.Size([20856, 1]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convirtiendo los datos a tensores de PyTorch\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).view(-1, 1)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values).view(-1, 1)\n",
    "\n",
    "# Creando conjuntos de datos y cargadores de datos\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "X_train_tensor.shape, y_train_tensor.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClassificationNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train.shape[1], 64)  # Capa oculta\n",
    "        self.fc2 = nn.Linear(64, 11)  # Capa de salida con 11 neuronas\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_indices = (y_train / 10).astype(int)\n",
    "y_test_indices = (y_test / 10).astype(int)\n",
    "\n",
    "y_train_tensor = torch.LongTensor(y_train_indices.values)\n",
    "y_test_tensor = torch.LongTensor(y_test_indices.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Crear un SummaryWriter para escribir a la carpeta './runs'\n",
    "writer = SummaryWriter('runs/classification_nn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion):\n",
    "    \"\"\"\n",
    "    Evalúa el modelo en el conjunto de validación.\n",
    "    \"\"\"\n",
    "    model.eval()  # Pone el modelo en modo de evaluación\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():  # Desactiva el cálculo de gradientes durante la evaluación\n",
    "        for data, target in test_loader:\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            total_loss += loss.item()\n",
    "    average_loss = total_loss / len(test_loader)\n",
    "    return average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tensor = torch.LongTensor(y_train.values)  # Not .view(-1, 1)\n",
    "y_test_tensor = torch.LongTensor(y_test.values)    # Not .view(-1, 1)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Training Loss: 1.1935, Validation Loss: 0.9764\n",
      "Epoch [101/1000], Training Loss: 1.1496, Validation Loss: 0.9473\n",
      "Epoch [201/1000], Training Loss: 0.9797, Validation Loss: 0.9507\n",
      "Epoch [301/1000], Training Loss: 0.8369, Validation Loss: 0.9528\n",
      "Epoch [401/1000], Training Loss: 0.8397, Validation Loss: 0.9562\n",
      "Epoch [501/1000], Training Loss: 1.0711, Validation Loss: 0.9604\n",
      "Epoch [601/1000], Training Loss: 0.9531, Validation Loss: 0.9631\n",
      "Epoch [701/1000], Training Loss: 1.0963, Validation Loss: 0.9653\n",
      "Epoch [801/1000], Training Loss: 1.0105, Validation Loss: 0.9664\n",
      "Epoch [901/1000], Training Loss: 0.9771, Validation Loss: 0.9695\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "# Training loop modificado para incluir \"validation loss\":\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Pone el modelo en modo de entrenamiento\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluar el \"validation loss\"\n",
    "    val_loss = evaluate_model(model, test_loader, criterion)\n",
    "\n",
    "    # Imprimir la pérdida cada 10 épocas\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Escribir en TensorBoard\n",
    "        writer.add_scalar('training loss', loss.item(), global_step= epoch)\n",
    "        writer.add_scalar('validation loss', val_loss, global_step=epoch)\n",
    "        writer.add_scalars('Losses', {\n",
    "        'Training Loss': loss.item(),\n",
    "        'Validation Loss': val_loss\n",
    "        }, global_step=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>growth_stage</th>\n",
       "      <th>damage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>DR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  growth_stage damage\n",
       "0            F     DR"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluar un dato\n",
    "# Assuming `data` is a single data point and `target` is the corresponding target value\n",
    "\n",
    "ejemplo = {\n",
    "   # \"filename\": \"L355F02268C02S08916Rp30595.jpg\",\n",
    "    \"growth_stage\": \"F\",\n",
    "    \"damage\":\"DR\",\n",
    "   # \"season\":\"LR2021\"\n",
    "}\n",
    "\n",
    "new_data = pd.DataFrame([ejemplo])\n",
    "new_data.head()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data_encoded = encoder.transform(new_data)\n",
    "\n",
    "new_data_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10])\n"
     ]
    }
   ],
   "source": [
    "# 1. Preprocesar el nuevo dato\n",
    "\n",
    "# Realizar one-hot encoding con el encoder ya entrenado\n",
    "new_data_encoded = encoder.transform(new_data)\n",
    "\n",
    "# Convertir el numpy array a un tensor de PyTorch\n",
    "new_data_tensor = torch.FloatTensor(new_data_encoded)\n",
    "\n",
    "# 2. Realizar una predicción con el modelo\n",
    "\n",
    "# Por ejemplo, para evaluar:\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(new_data_tensor)\n",
    "    probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1)\n",
    "    predicted_label = predicted_class * 10  # Convertir índice de clase nuevamente a la etiqueta original\n",
    "    print(predicted_label)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
